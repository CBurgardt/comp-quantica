{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação com Redes Neurais Quânticas: Teoria e Prática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "Apresentar as justificativas e o propósito deste trabalho (não deve ser muito longo).\n",
    "\n",
    "Explicar sua estrutura.\n",
    "\n",
    "Indicar os objetivos desejados e os alcançados (spoilers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificação de Dados\n",
    "Explicar como o estado inicial pode representar o dado a ser classificado.\n",
    "\n",
    "Avaliar se é conveniente incluir outros tipos de codificação além de qubit encoding, como, por exemplo, oráculo, amplitude encoding ou dados quânticos.\n",
    "\n",
    "$z=z_1 z_2 \\dotsb z_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura de Circuitos\n",
    "\n",
    "O estado de entrada é preparado e então tranformado através de uma sequência de operações unitárias que dependem de um parâmetro (real ou complexo?).\n",
    "\n",
    "O circuito é composto por n+1 qubits - ignorando possíveis ancillas. O último qubit será utilizado para medição e leitura da saída.\n",
    "\n",
    "$\\{ U_a(\\theta) \\}$\n",
    "\n",
    "$U(\\vec{\\theta})=U_L(\\theta_L)U_{L-1}(\\theta_{L-1}) \\dotsb U_1(\\theta_1)$\n",
    "\n",
    "$\\left|z,1\\right\\rangle=\\left|z_1 z_2 \\dotsb z_n,1\\right\\rangle$\n",
    "\n",
    "$U(\\vec{\\theta})\\left|z,1\\right\\rangle$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrização\n",
    "\n",
    "Citar os três tipos de parametrização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representação\n",
    " \n",
    "Queremos demonstrar que o modelo de rede neural quântica aqui apresentado é capaz de expressar qualquer função de rótulo com dois valores, mesmo que, possivelmente, com um alto custo em termos de profundidade do circuito.\n",
    "\n",
    "Para n bits, temos $2^n$ strings e, portanto, $2^{(2^n)}$ possíveis funções de rótulo $l(z)$. Dada uma função de rótulo, considere o operador cuja ação é definida nos estados da base computacional como\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "U_l \\left|z, z_{n+1}\\right\\rangle = e^{i \\frac{\\pi}{4} l(z) X_{n+1}} \\left|z,z_{n+1}\\right\\rangle\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "que é a rotação de $\\frac{\\pi}{4}$ do qubit de saída ($z_{n+1}$) sobre o eixo $x$. De maneira equivalente\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U_l^\\dagger Y_{n+1} U_l\n",
    "&=\\left( e^{-i \\frac{\\pi}{4} l(z) X_{n+1}} \\right) Y_{n+1} \\left( e^{i \\frac{\\pi}{4} l(z) X_{n+1}} \\right) \\\\\n",
    "&=\\left[ cos \\left(\\frac{\\pi}{4}l(Z) \\right)I - i sen \\left(\\frac{\\pi}{4}l(Z) \\right)X_{n+1} \\right] Y_{n+1} \\left[ cos \\left(\\frac{\\pi}{4}l(Z) \\right)I + i sen \\left(\\frac{\\pi}{4}l(Z) \\right)X_{n+1} \\right] \\\\\n",
    "&=cos \\left(\\frac{\\pi}{2}l(Z) \\right)Y_{n+1} + sen \\left(\\frac{\\pi}{2}l(Z) \\right)Z_{n+1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "onde $l(Z)$ é um operador diagonal na base computacional. Sabendo que $l(z)=+1, -1$, podemos mostrar que\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left\\langle z,1\\right| U_l^\\dagger Y_{n+1} U_l \\left|z,1\\right\\rangle \n",
    "&=cos \\left(\\frac{\\pi}{2}l(Z) \\right)\\left\\langle z,1\\right| Y_{n+1}\\left|z,1\\right\\rangle + sen \\left(\\frac{\\pi}{2}l(Z) \\right) \\left\\langle z,1\\right| Z_{n+1}\\left|z,1\\right\\rangle \\\\\n",
    "&=sen \\left(\\frac{\\pi}{2}l(Z) \\right) \\left\\langle z,1\\right| Z_{n+1}\\left|z,1\\right\\rangle \\\\\n",
    "&=l(z)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Vemos então, de maneira abstrata, que temos uma forma de representar qualquer rótulo através de um circuito quântico.\n",
    "\n",
    "Vamos agora explicar como escrever $U_l$ como um produto de uma operação unitária em dois qubits. Para esta discussão é conveniente mudarmos para variáveis Booleanas $b_i=\\frac{1}{2}(1-z_i)$ e pensar em nossa função de rótulo $l$ como $1-2b$, onde $b$ é 0 ou 1. Podemos agora usar a representação de Reed-Muller para uma função Booleana em termos dos bits $b_1$ a $b_n$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "b=a_0\\oplus (a_1 b_1 \\oplus a_2 b_2 \\oplus \\dotsb a_n b_n)\\oplus (a_{12} b_1 b_2 \\oplus a_{13} b_1 b_3\\oplus \\dotsb)\\oplus \\dotsb \\oplus a_{123} \\dotsb b_1 b_2\\dotsb b_n .\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "A adição é mod2 e os coeficiente $a$ são todos 0 ou 1. Perceba que há $2^n$ coeficientes e, sendo todos 0 ou 1, vemos que efetivamente que há $2^{(2^n)}$ funções Booleanas sendo representadas. A fórmula pode ser exponenciamente longa. Agora podemos escrever a operação unitária dependente da função de rótulo como:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U_l \n",
    "&=e^{i \\frac{\\pi}{4}l(z)X_{n+1}} \\\\\n",
    "&=e^{i \\frac{\\pi}{4}(1-2B)X_{n+1}} \\\\\n",
    "&=e^{i \\frac{\\pi}{4}X_{n+1}}e^{-i \\frac{\\pi}{2} B X_{n+1}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "onde $B$ é o operador, diagonal na base computacional, correspodente a $b$. Cada termo de $B$ é multiplicado por $X_{n+1}$ e, portanto, cada termo comuta com os outros. Cada termos não nulo na fórmula de Reed-Muller dá origem em $U_l$ a um bit flip (NOT) controlado no qubit de saída. Para ver isso, considere o termo de três bits envolvendo os bits 2, 7 e 9. Ele corresponde ao operador\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "e^{-i\\frac{\\pi}{2}B_2 B_7 B_9 X_{n+1}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "o qual, agindo num estado da base computacional nos primeiros n qubits, é a identidade, a menos que $b_2=b_7=b_9=1$ quando é $-i X_{n+1}$. É conhecido de outros trabalhos [11] que qualquer operação unitária controlada atuando no qubit $n+1$, onde o controle é feito pelos primeiros n qubits, pode ser escrito como um produto de $n^2$ operações unitárias em dois qubits. Portanto, qualquer função de rótulo expressa em termos da fórmula de Reed-Muller com M termos pode ser escrita como o produto de operadores unitários, que comutam, em $n+1$ qubits, e cada um deles pode ser escrito com $n^2$ operações unitárias em dois qubits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizado\n",
    "\n",
    "$loss(\\vec{\\theta}, z)=1-l(z)M_\\theta(\\left|z,1\\right\\rangle)$\n",
    "\n",
    "$loss(\\vec{\\theta}, z)=\\frac{1}{D} \\sum_{d=1}^{D} (M_\\theta(\\left|z,1\\right\\rangle^d) - l(z)^d)^2$\n",
    "\n",
    "$M_\\theta(\\left|z,1\\right\\rangle) = \\left\\langle z,1\\right| U^\\dagger(\\vec{\\theta}) Y_{n+1} U(\\vec{\\theta}) \\left|z,1\\right\\rangle$\n",
    "\n",
    "### Stochastic Gradient Descent Algorithm\n",
    "#### Variar um componente do gradiente por vez\n",
    "\n",
    "$\\vec{\\theta^{(t+1)}} \\leftarrow{} \\vec{\\theta^{(t)}} + \\eta^{(t)} \\nabla loss^{(t)}$\n",
    "\n",
    "$\\frac{df}{dx}(x) = \\left( f(x+\\epsilon)-f(x-\\epsilon) \\right) / (2\\epsilon) + O(\\epsilon^2)$\n",
    "\n",
    "#### Estimar quânticamente\n",
    "\n",
    "$\\frac{d loss(\\vec{\\theta},z))}{d\\theta_k} = 2Im\\left( \\left\\langle z,1\\right| U^\\dagger_1 \\dotsb U^\\dagger_L Y_{n+1} U_L \\dotsb U_{k+1} \\Sigma_k U_k \\dotsb U_1 \\left|z,1\\right\\rangle \\right)$\n",
    "\n",
    "$u(\\vec(\\theta)) = U^\\dagger_1 \\dotsb U^\\dagger_L Y_{n+1} U_L \\dotsb U_{k+1} \\Sigma_k U_k \\dotsb U_1 $\n",
    "\n",
    "$\\frac{d loss(\\vec{\\theta},z))}{d\\theta_k} = 2Im\\left( \\left\\langle z,1\\right| u \\left|z,1\\right\\rangle \\right)$\n",
    "\n",
    "\n",
    "### Adaptive Moment Estimation (ADAM)\n",
    "\n",
    "Resumo curto.\n",
    "\n",
    "Foi utilizado no artigo \"Hierarchical quantum classifiers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prática: TTN\n",
    "\n",
    "<img src=\"https://github.com/israelferrazaraujo/comp-quantica/blob/patch-3/projetos/2019-01/files/ClassicalLearning.png?raw=true\">\n",
    "\n",
    "<img src=\"https://github.com/israelferrazaraujo/comp-quantica/blob/patch-3/projetos/2019-01/files/QuantumGradient.png?raw=true\">\n",
    "\n",
    "### Clássico\n",
    "Resumo curto sobre TTN (Tree Tensor Network)\n",
    "### Quântico\n",
    "Versão quântica da TTN\n",
    "### Qiskit\n",
    "Implementação utilizando Python e Qiskit\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "\n",
    "https://scikit-learn.org/0.19/datasets/mldata.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets",
    "\n",
    "<img src=\"https://github.com/israelferrazaraujo/comp-quantica/blob/patch-3/projetos/2019-01/files/HierarchicalQuantumClassifier.png?raw=true\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração:  1\n",
      "LOSS: 0.56\tTgt: 0  Cnt: {'0': 32, '1': 96} ERR: 0.25 %\n",
      "learning...\n",
      "LOSS: 0.71\tTgt: 0  Cnt: {'0': 20, '1': 108} ERR: 0.16 %\n",
      "learning...\n",
      "LOSS: 0.81\tTgt: 1  Cnt: {'0': 115, '1': 13} ERR: 0.10 %\n",
      "learning...\n",
      "LOSS: 0.17\tTgt: 1  Cnt: {'0': 53, '1': 75} ERR: 0.59 %\n",
      "learning...\n",
      "LOSS: 0.27\tTgt: 0  Cnt: {'0': 61, '1': 67} ERR: 0.48 %\n",
      "learning...\n",
      "LOSS: 0.70\tTgt: 1  Cnt: {'0': 107, '1': 21} ERR: 0.16 %\n",
      "learning...\n",
      "LOSS: 0.17\tTgt: 0  Cnt: {'0': 75, '1': 53} ERR: 0.59 %\n",
      "learning...\n",
      "LOSS: 0.45\tTgt: 0  Cnt: {'0': 42, '1': 86} ERR: 0.33 %\n",
      "learning...\n",
      "LOSS: 0.12\tTgt: 1  Cnt: {'0': 44, '1': 84} ERR: 0.66 %\n",
      "learning...\n",
      "LOSS: 0.65\tTgt: 0  Cnt: {'0': 25, '1': 103} ERR: 0.20 %\n",
      "learning...\n",
      "Iteração:  2\n",
      "LOSS: 0.66\tTgt: 0  Cnt: {'0': 24, '1': 104} ERR: 0.19 %\n",
      "learning...\n",
      "LOSS: 0.48\tTgt: 0  Cnt: {'0': 39, '1': 89} ERR: 0.30 %\n",
      "learning...\n",
      "LOSS: 0.10\tTgt: 1  Cnt: {'0': 40, '1': 88} ERR: 0.69 %\n",
      "LOSS: 0.48\tTgt: 1  Cnt: {'0': 89, '1': 39} ERR: 0.30 %\n",
      "learning...\n",
      "LOSS: 0.94\tTgt: 0  Cnt: {'0': 4, '1': 124} ERR: 0.03 %\n",
      "learning...\n",
      "LOSS: 0.29\tTgt: 1  Cnt: {'0': 69, '1': 59} ERR: 0.46 %\n",
      "learning...\n",
      "LOSS: 0.00\tTgt: 0  Cnt: {'0': 120, '1': 8} OK : 0.94 %\n",
      "LOSS: 0.02\tTgt: 0  Cnt: {'0': 111, '1': 17} OK : 0.87 %\n",
      "LOSS: 0.52\tTgt: 1  Cnt: {'0': 92, '1': 36} ERR: 0.28 %\n",
      "learning...\n",
      "LOSS: 0.10\tTgt: 0  Cnt: {'0': 88, '1': 40} ERR: 0.69 %\n",
      "Iteração:  3\n",
      "LOSS: 0.17\tTgt: 0  Cnt: {'0': 76, '1': 52} ERR: 0.59 %\n",
      "learning...\n",
      "LOSS: 0.08\tTgt: 0  Cnt: {'0': 91, '1': 37} ERR: 0.71 %\n",
      "LOSS: 0.14\tTgt: 1  Cnt: {'0': 48, '1': 80} ERR: 0.62 %\n",
      "learning...\n",
      "LOSS: 0.33\tTgt: 1  Cnt: {'0': 74, '1': 54} ERR: 0.42 %\n",
      "learning...\n",
      "LOSS: 0.34\tTgt: 0  Cnt: {'0': 53, '1': 75} ERR: 0.41 %\n",
      "learning...\n",
      "LOSS: 0.30\tTgt: 1  Cnt: {'0': 70, '1': 58} ERR: 0.45 %\n",
      "learning...\n",
      "LOSS: 0.10\tTgt: 0  Cnt: {'0': 88, '1': 40} ERR: 0.69 %\n",
      "LOSS: 0.09\tTgt: 0  Cnt: {'0': 90, '1': 38} ERR: 0.70 %\n",
      "LOSS: 0.23\tTgt: 1  Cnt: {'0': 61, '1': 67} ERR: 0.52 %\n",
      "learning...\n",
      "LOSS: 0.34\tTgt: 0  Cnt: {'0': 53, '1': 75} ERR: 0.41 %\n",
      "learning...\n",
      "Iteração:  4\n",
      "LOSS: 0.74\tTgt: 0  Cnt: {'0': 18, '1': 110} ERR: 0.14 %\n",
      "learning...\n",
      "LOSS: 0.82\tTgt: 0  Cnt: {'0': 12, '1': 116} ERR: 0.09 %\n",
      "learning...\n",
      "LOSS: 0.21\tTgt: 1  Cnt: {'0': 58, '1': 70} ERR: 0.55 %\n",
      "learning...\n",
      "LOSS: 0.09\tTgt: 1  Cnt: {'0': 39, '1': 89} ERR: 0.70 %\n",
      "LOSS: 0.61\tTgt: 0  Cnt: {'0': 28, '1': 100} ERR: 0.22 %\n",
      "learning...\n",
      "LOSS: 0.03\tTgt: 1  Cnt: {'0': 21, '1': 107} OK : 0.84 %\n",
      "LOSS: 0.33\tTgt: 0  Cnt: {'0': 54, '1': 74} ERR: 0.42 %\n",
      "learning...\n",
      "LOSS: 0.34\tTgt: 0  Cnt: {'0': 53, '1': 75} ERR: 0.41 %\n",
      "learning...\n",
      "LOSS: 0.52\tTgt: 1  Cnt: {'0': 92, '1': 36} ERR: 0.28 %\n",
      "learning...\n",
      "LOSS: 0.17\tTgt: 0  Cnt: {'0': 76, '1': 52} ERR: 0.59 %\n",
      "learning...\n",
      "Iteração:  5\n",
      "LOSS: 0.28\tTgt: 0  Cnt: {'0': 60, '1': 68} ERR: 0.47 %\n",
      "learning...\n",
      "LOSS: 0.23\tTgt: 0  Cnt: {'0': 67, '1': 61} ERR: 0.52 %\n",
      "learning...\n",
      "LOSS: 0.28\tTgt: 1  Cnt: {'0': 68, '1': 60} ERR: 0.47 %\n",
      "learning...\n",
      "LOSS: 0.20\tTgt: 1  Cnt: {'0': 57, '1': 71} ERR: 0.55 %\n",
      "learning...\n",
      "LOSS: 0.49\tTgt: 0  Cnt: {'0': 38, '1': 90} ERR: 0.30 %\n",
      "learning...\n",
      "LOSS: 0.17\tTgt: 1  Cnt: {'0': 53, '1': 75} ERR: 0.59 %\n",
      "learning...\n",
      "LOSS: 0.92\tTgt: 0  Cnt: {'0': 5, '1': 123} ERR: 0.04 %\n",
      "learning...\n",
      "LOSS: 0.34\tTgt: 0  Cnt: {'0': 53, '1': 75} ERR: 0.41 %\n",
      "learning...\n",
      "LOSS: 0.05\tTgt: 1  Cnt: {'0': 30, '1': 98} OK : 0.77 %\n",
      "LOSS: 0.18\tTgt: 0  Cnt: {'0': 74, '1': 54} ERR: 0.58 %\n",
      "learning...\n",
      "Iteração:  6\n",
      "LOSS: 0.04\tTgt: 0  Cnt: {'0': 103, '1': 25} OK : 0.80 %\n",
      "LOSS: 0.09\tTgt: 0  Cnt: {'0': 89, '1': 39} ERR: 0.70 %\n",
      "LOSS: 0.14\tTgt: 1  Cnt: {'0': 48, '1': 80} ERR: 0.62 %\n",
      "learning...\n",
      "LOSS: 0.67\tTgt: 1  Cnt: {'0': 105, '1': 23} ERR: 0.18 %\n",
      "learning...\n",
      "LOSS: 0.00\tTgt: 0  Cnt: {'0': 127, '1': 1} OK : 0.99 %\n",
      "LOSS: 0.65\tTgt: 1  Cnt: {'0': 103, '1': 25} ERR: 0.20 %\n",
      "learning...\n",
      "LOSS: 0.00\tTgt: 0  Cnt: {'0': 126, '1': 2} OK : 0.98 %\n",
      "LOSS: 0.00\tTgt: 0  Cnt: {'0': 123, '1': 5} OK : 0.96 %\n",
      "LOSS: 0.56\tTgt: 1  Cnt: {'0': 96, '1': 32} ERR: 0.25 %\n",
      "learning...\n",
      "LOSS: 0.57\tTgt: 0  Cnt: {'0': 31, '1': 97} ERR: 0.24 %\n",
      "learning...\n",
      "Iteração:  7\n",
      "LOSS: 0.70\tTgt: 0  Cnt: {'0': 21, '1': 107} ERR: 0.16 %\n",
      "learning...\n",
      "LOSS: 0.00\tTgt: 0  Cnt: {'0': 121, '1': 7} OK : 0.95 %\n",
      "LOSS: 0.57\tTgt: 1  Cnt: {'0': 97, '1': 31} ERR: 0.24 %\n",
      "learning...\n",
      "LOSS: 0.60\tTgt: 1  Cnt: {'0': 99, '1': 29} ERR: 0.23 %\n",
      "learning...\n",
      "LOSS: 0.07\tTgt: 0  Cnt: {'0': 95, '1': 33} ERR: 0.74 %\n",
      "LOSS: 0.24\tTgt: 1  Cnt: {'0': 63, '1': 65} ERR: 0.51 %\n",
      "learning...\n",
      "LOSS: 0.05\tTgt: 0  Cnt: {'0': 98, '1': 30} OK : 0.77 %\n",
      "LOSS: 0.05\tTgt: 0  Cnt: {'0': 100, '1': 28} OK : 0.78 %\n",
      "LOSS: 0.41\tTgt: 1  Cnt: {'0': 82, '1': 46} ERR: 0.36 %\n",
      "learning...\n",
      "LOSS: 0.06\tTgt: 0  Cnt: {'0': 97, '1': 31} OK : 0.76 %\n",
      "Iteração:  8\n",
      "LOSS: 0.08\tTgt: 0  Cnt: {'0': 92, '1': 36} ERR: 0.72 %\n",
      "LOSS: 0.04\tTgt: 0  Cnt: {'0': 104, '1': 24} OK : 0.81 %\n",
      "LOSS: 0.30\tTgt: 1  Cnt: {'0': 70, '1': 58} ERR: 0.45 %\n",
      "learning...\n",
      "LOSS: 0.75\tTgt: 1  Cnt: {'0': 111, '1': 17} ERR: 0.13 %\n",
      "learning...\n",
      "LOSS: 0.01\tTgt: 0  Cnt: {'0': 118, '1': 10} OK : 0.92 %\n",
      "LOSS: 0.46\tTgt: 1  Cnt: {'0': 87, '1': 41} ERR: 0.32 %\n",
      "learning...\n",
      "LOSS: 0.06\tTgt: 0  Cnt: {'0': 96, '1': 32} OK : 0.75 %\n",
      "LOSS: 0.12\tTgt: 0  Cnt: {'0': 84, '1': 44} ERR: 0.66 %\n",
      "learning...\n",
      "LOSS: 0.33\tTgt: 1  Cnt: {'0': 73, '1': 55} ERR: 0.43 %\n",
      "learning...\n",
      "LOSS: 0.00\tTgt: 0  Cnt: {'0': 121, '1': 7} OK : 0.95 %\n",
      "Iteração:  9\n",
      "LOSS: 0.01\tTgt: 0  Cnt: {'0': 115, '1': 13} OK : 0.90 %\n",
      "LOSS: 0.04\tTgt: 0  Cnt: {'0': 102, '1': 26} OK : 0.80 %\n",
      "LOSS: 0.44\tTgt: 1  Cnt: {'0': 85, '1': 43} ERR: 0.34 %\n",
      "learning...\n",
      "LOSS: 0.53\tTgt: 1  Cnt: {'0': 93, '1': 35} ERR: 0.27 %\n",
      "learning...\n",
      "LOSS: 0.01\tTgt: 0  Cnt: {'0': 117, '1': 11} OK : 0.91 %\n",
      "LOSS: 0.23\tTgt: 1  Cnt: {'0': 62, '1': 66} ERR: 0.52 %\n",
      "learning...\n",
      "LOSS: 0.02\tTgt: 0  Cnt: {'0': 108, '1': 20} OK : 0.84 %\n",
      "LOSS: 0.01\tTgt: 0  Cnt: {'0': 113, '1': 15} OK : 0.88 %\n",
      "LOSS: 0.75\tTgt: 1  Cnt: {'0': 111, '1': 17} ERR: 0.13 %\n",
      "learning...\n",
      "LOSS: 0.10\tTgt: 0  Cnt: {'0': 88, '1': 40} ERR: 0.69 %\n",
      "Iteração:  10\n",
      "LOSS: 0.04\tTgt: 0  Cnt: {'0': 101, '1': 27} OK : 0.79 %\n",
      "LOSS: 0.12\tTgt: 0  Cnt: {'0': 84, '1': 44} ERR: 0.66 %\n",
      "learning...\n",
      "LOSS: 0.18\tTgt: 1  Cnt: {'0': 55, '1': 73} ERR: 0.57 %\n",
      "learning...\n",
      "LOSS: 0.48\tTgt: 1  Cnt: {'0': 89, '1': 39} ERR: 0.30 %\n",
      "learning...\n",
      "LOSS: 0.65\tTgt: 0  Cnt: {'0': 25, '1': 103} ERR: 0.20 %\n",
      "learning...\n",
      "LOSS: 0.81\tTgt: 1  Cnt: {'0': 115, '1': 13} ERR: 0.10 %\n",
      "learning...\n",
      "LOSS: 0.74\tTgt: 0  Cnt: {'0': 18, '1': 110} ERR: 0.14 %\n",
      "learning...\n",
      "LOSS: 0.26\tTgt: 0  Cnt: {'0': 63, '1': 65} ERR: 0.49 %\n",
      "learning...\n",
      "LOSS: 0.61\tTgt: 1  Cnt: {'0': 100, '1': 28} ERR: 0.22 %\n",
      "learning...\n",
      "LOSS: 0.78\tTgt: 0  Cnt: {'0': 15, '1': 113} ERR: 0.12 %\n",
      "learning...\n",
      "Iteração:  11\n",
      "LOSS: 0.06\tTgt: 0  Cnt: {'0': 97, '1': 31} OK : 0.76 %\n",
      "LOSS: 0.01\tTgt: 0  Cnt: {'0': 117, '1': 11} OK : 0.91 %\n",
      "LOSS: 0.44\tTgt: 1  Cnt: {'0': 85, '1': 43} ERR: 0.34 %\n",
      "learning...\n",
      "LOSS: 0.65\tTgt: 1  Cnt: {'0': 103, '1': 25} ERR: 0.20 %\n",
      "learning...\n",
      "LOSS: 0.29\tTgt: 0  Cnt: {'0': 59, '1': 69} ERR: 0.46 %\n",
      "learning...\n",
      "LOSS: 0.09\tTgt: 1  Cnt: {'0': 38, '1': 90} ERR: 0.70 %\n",
      "LOSS: 0.42\tTgt: 0  Cnt: {'0': 45, '1': 83} ERR: 0.35 %\n",
      "learning...\n",
      "LOSS: 0.75\tTgt: 0  Cnt: {'0': 17, '1': 111} ERR: 0.13 %\n",
      "learning...\n",
      "LOSS: 0.42\tTgt: 1  Cnt: {'0': 83, '1': 45} ERR: 0.35 %\n",
      "learning...\n",
      "LOSS: 0.35\tTgt: 0  Cnt: {'0': 52, '1': 76} ERR: 0.41 %\n",
      "learning...\n",
      "Iteração:  12\n",
      "LOSS: 0.21\tTgt: 0  Cnt: {'0': 69, '1': 59} ERR: 0.54 %\n",
      "learning...\n",
      "LOSS: 0.73\tTgt: 0  Cnt: {'0': 19, '1': 109} ERR: 0.15 %\n",
      "learning...\n",
      "LOSS: 0.12\tTgt: 1  Cnt: {'0': 45, '1': 83} ERR: 0.65 %\n",
      "learning...\n",
      "LOSS: 0.48\tTgt: 1  Cnt: {'0': 89, '1': 39} ERR: 0.30 %\n",
      "learning...\n",
      "LOSS: 0.62\tTgt: 0  Cnt: {'0': 27, '1': 101} ERR: 0.21 %\n",
      "learning...\n",
      "LOSS: 0.81\tTgt: 1  Cnt: {'0': 115, '1': 13} ERR: 0.10 %\n",
      "learning...\n",
      "LOSS: 0.35\tTgt: 0  Cnt: {'0': 52, '1': 76} ERR: 0.41 %\n",
      "learning...\n",
      "LOSS: 0.27\tTgt: 0  Cnt: {'0': 62, '1': 66} ERR: 0.48 %\n",
      "learning...\n",
      "LOSS: 0.39\tTgt: 1  Cnt: {'0': 80, '1': 48} ERR: 0.38 %\n",
      "learning...\n",
      "LOSS: 0.38\tTgt: 0  Cnt: {'0': 49, '1': 79} ERR: 0.38 %\n",
      "learning...\n",
      "Iteração:  13\n",
      "LOSS: 0.84\tTgt: 0  Cnt: {'0': 11, '1': 117} ERR: 0.09 %\n",
      "learning...\n",
      "LOSS: 0.02\tTgt: 0  Cnt: {'0': 109, '1': 19} OK : 0.85 %\n",
      "LOSS: 0.07\tTgt: 1  Cnt: {'0': 35, '1': 93} ERR: 0.73 %\n",
      "LOSS: 0.49\tTgt: 1  Cnt: {'0': 90, '1': 38} ERR: 0.30 %\n",
      "learning...\n",
      "LOSS: 0.12\tTgt: 0  Cnt: {'0': 84, '1': 44} ERR: 0.66 %\n",
      "learning...\n",
      "LOSS: 0.03\tTgt: 1  Cnt: {'0': 23, '1': 105} OK : 0.82 %\n",
      "LOSS: 0.06\tTgt: 0  Cnt: {'0': 97, '1': 31} OK : 0.76 %\n",
      "LOSS: 0.05\tTgt: 0  Cnt: {'0': 100, '1': 28} OK : 0.78 %\n",
      "LOSS: 0.00\tTgt: 1  Cnt: {'0': 5, '1': 123} OK : 0.96 %\n",
      "LOSS: 0.02\tTgt: 0  Cnt: {'0': 112, '1': 16} OK : 0.88 %\n",
      "Iteração:  14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.03\tTgt: 0  Cnt: {'0': 105, '1': 23} OK : 0.82 %\n",
      "LOSS: 0.04\tTgt: 0  Cnt: {'0': 102, '1': 26} OK : 0.80 %\n",
      "LOSS: 0.00\tTgt: 1  Cnt: {'0': 5, '1': 123} OK : 0.96 %\n",
      "LOSS: 0.15\tTgt: 1  Cnt: {'0': 50, '1': 78} ERR: 0.61 %\n",
      "learning...\n",
      "LOSS: 0.13\tTgt: 0  Cnt: {'0': 82, '1': 46} ERR: 0.64 %\n",
      "learning...\n",
      "LOSS: 0.03\tTgt: 1  Cnt: {'0': 22, '1': 106} OK : 0.83 %\n",
      "LOSS: 0.26\tTgt: 0  Cnt: {'0': 63, '1': 65} ERR: 0.49 %\n",
      "learning...\n",
      "LOSS: 0.21\tTgt: 0  Cnt: {'0': 69, '1': 59} ERR: 0.54 %\n",
      "learning...\n",
      "LOSS: 0.17\tTgt: 1  Cnt: {'0': 52, '1': 76} ERR: 0.59 %\n",
      "learning...\n",
      "LOSS: 0.01\tTgt: 0  Cnt: {'0': 116, '1': 12} OK : 0.91 %\n",
      "Iteração:  15\n",
      "LOSS: 0.01\tTgt: 0  Cnt: {'0': 115, '1': 13} OK : 0.90 %\n",
      "LOSS: 0.01\tTgt: 0  Cnt: {'0': 113, '1': 15} OK : 0.88 %\n",
      "LOSS: 0.04\tTgt: 1  Cnt: {'0': 25, '1': 103} OK : 0.80 %\n",
      "LOSS: 0.13\tTgt: 1  Cnt: {'0': 47, '1': 81} ERR: 0.63 %\n",
      "learning...\n",
      "LOSS: 0.03\tTgt: 0  Cnt: {'0': 107, '1': 21} OK : 0.84 %\n",
      "LOSS: 0.00\tTgt: 1  Cnt: {'0': 7, '1': 121} OK : 0.95 %\n",
      "LOSS: 0.04\tTgt: 0  Cnt: {'0': 103, '1': 25} OK : 0.80 %\n",
      "LOSS: 0.02\tTgt: 0  Cnt: {'0': 108, '1': 20} OK : 0.84 %\n",
      "LOSS: 0.04\tTgt: 1  Cnt: {'0': 24, '1': 104} OK : 0.81 %\n",
      "LOSS: 0.01\tTgt: 0  Cnt: {'0': 117, '1': 11} OK : 0.91 %\n",
      "Iteração:  16\n",
      "LOSS: 0.04\tTgt: 0  Cnt: {'0': 102, '1': 26} OK : 0.80 %\n",
      "LOSS: 0.01\tTgt: 0  Cnt: {'0': 116, '1': 12} OK : 0.91 %\n",
      "LOSS: 0.04\tTgt: 1  Cnt: {'0': 24, '1': 104} OK : 0.81 %\n",
      "LOSS: 0.03\tTgt: 1  Cnt: {'0': 21, '1': 107} OK : 0.84 %\n",
      "LOSS: 0.03\tTgt: 0  Cnt: {'0': 107, '1': 21} OK : 0.84 %\n",
      "LOSS: 0.01\tTgt: 1  Cnt: {'0': 11, '1': 117} OK : 0.91 %\n",
      "LOSS: 0.02\tTgt: 0  Cnt: {'0': 110, '1': 18} OK : 0.86 %\n",
      "LOSS: 0.05\tTgt: 0  Cnt: {'0': 98, '1': 30} OK : 0.77 %\n",
      "LOSS: 0.05\tTgt: 1  Cnt: {'0': 30, '1': 98} OK : 0.77 %\n",
      "LOSS: 0.01\tTgt: 0  Cnt: {'0': 117, '1': 11} OK : 0.91 %\n",
      "Parâmetros:\n",
      "[9.478058508573541, 4.2864228056935865, 10.821199813661151, -6.513706476987668] \t[-1.9989006558309976, 0.641220569578127] \t[-2.741309737549505] \t\tTgt: 1  Cnt: {'0': 18, '1': 110} OK : 0.86 %\n",
      "\tTgt: 0  Cnt: {'0': 103, '1': 25} OK : 0.80 %\n",
      "\tTgt: 1  Cnt: {'0': 7, '1': 121} OK : 0.95 %\n",
      "\tTgt: 1  Cnt: {'0': 17, '1': 111} OK : 0.87 %\n",
      "\tTgt: 0  Cnt: {'0': 111, '1': 17} OK : 0.87 %\n",
      "\tTgt: 1  Cnt: {'0': 12, '1': 116} OK : 0.91 %\n",
      "\tTgt: 0  Cnt: {'0': 112, '1': 16} OK : 0.88 %\n",
      "\tTgt: 0  Cnt: {'0': 122, '1': 6} OK : 0.95 %\n",
      "\tTgt: 0  Cnt: {'0': 111, '1': 17} OK : 0.87 %\n",
      "\tTgt: 1  Cnt: {'0': 12, '1': 116} OK : 0.91 %\n"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from sklearn import datasets\n",
    "from qiskit import(\n",
    "    QuantumCircuit,\n",
    "    QuantumRegister,\n",
    "    ClassicalRegister,\n",
    "    execute,\n",
    "    Aer,\n",
    "    aqua)\n",
    "from qiskit.visualization import plot_histogram\n",
    "\n",
    "\n",
    "def qubit_encoding(circuit, q, classical_data, dagger=False):\n",
    "    '''\n",
    "    Codifica a informação clássica no estado inicial usando o operador RY.\n",
    "    '''\n",
    "    f = 1\n",
    "    if (dagger):\n",
    "        f = -1\n",
    "\n",
    "    for i in range(len(classical_data)):\n",
    "        circuit.ry(classical_data[i]*f, q[i])\n",
    "\n",
    "\n",
    "def mry(circuit, q, params, dagger=False):\n",
    "    '''\n",
    "    mry = Multiple RY.\n",
    "    Função recursiva.\n",
    "    Aplica a porta RY num conjunto de qubits e os conecta usando CNOT.\n",
    "    '''\n",
    "    f = 1\n",
    "    if (dagger):\n",
    "        f = -1\n",
    "\n",
    "    n = len(q)\n",
    "    if n < 3:\n",
    "        circuit.ry(params[0]*f, q[0])\n",
    "        if n > 1:\n",
    "            circuit.ry(params[1]*f, q[1])\n",
    "            circuit.cx(q[1], q[0])\n",
    "    else:\n",
    "        k = m.ceil(n / 2)\n",
    "        mry(circuit, q[:k], params[:k])\n",
    "        mry(circuit, q[k:], params[k:])\n",
    "\n",
    "\n",
    "def quantum_processing(classical_data, parameters, draw_circuit=False, draw_filename=None):\n",
    "    '''\n",
    "    Algorítmo quântico.\n",
    "    Monta a rede neural quântica para classificar a informação clássica (classical_data).\n",
    "    '''\n",
    "    n = len(classical_data)\n",
    "\n",
    "    q = QuantumRegister(n, \"q\")\n",
    "    c = ClassicalRegister(1, \"c\")\n",
    "    circuit = QuantumCircuit(q, c)\n",
    "    \n",
    "    qubit_encoding(circuit, q, classical_data)\n",
    "    circuit.barrier()\n",
    "\n",
    "    layer = 0\n",
    "    while True:\n",
    "        ops_count = m.ceil(n / 2**layer)\n",
    "\n",
    "        mry(circuit, q[:n:m.ceil(n / ops_count)], parameters[layer])\n",
    "        circuit.barrier()\n",
    "\n",
    "        layer += 1\n",
    "        if (ops_count <= 1):\n",
    "            break\n",
    "\n",
    "    #circuit.rx(-m.pi/2, q[0])\n",
    "    circuit.measure(q[0], c[0])\n",
    "\n",
    "    simulator = Aer.get_backend('qasm_simulator')\n",
    "    job = execute(circuit, simulator, shots=128)\n",
    "    result = job.result()\n",
    "    counts = result.get_counts(circuit)\n",
    "\n",
    "    if (draw_circuit):\n",
    "        circuit.draw(output='latex', filename=draw_filename)\n",
    "        \n",
    "    return counts\n",
    "\n",
    "\n",
    "def load_iris_set(training_set_size, test_set_size, desired_targets):\n",
    "    ''' \n",
    "    Obtenção dos dados IRIS.\n",
    "    Os dados clássicos devem ser reescalados para o intervalo [0, pi].\n",
    "    '''\n",
    "    iris = datasets.load_iris()\n",
    "    zipped = list(zip(iris.data, iris.target))\n",
    "    data = np.array(rnd.choices(\n",
    "        [[*e[0], e[1]] for e in zipped if e[1] in desired_targets], k=training_set_size+test_set_size))\n",
    "    rows = len(data)\n",
    "    cols = len(data[0])\n",
    "    for i in range(cols - 1):  # -1 para ignorar a coluna target.\n",
    "        max_value = data[:, i].max()\n",
    "        for j in range(rows):\n",
    "            data[j, i] = m.pi * data[j, i] / max_value\n",
    "\n",
    "    return data[:training_set_size], data[training_set_size:]\n",
    "\n",
    "\n",
    "def initialize_unitary_parameters(qubits_count):\n",
    "    parameters = []\n",
    "    \n",
    "    layer = 0\n",
    "    while True:\n",
    "        ops_count = m.ceil(qubits_count / 2**layer)\n",
    "\n",
    "        parameters.append([i * m.pi for i in rand(ops_count)])\n",
    "\n",
    "        layer += 1\n",
    "        if (ops_count <= 1):\n",
    "            break\n",
    "\n",
    "    return parameters\n",
    "\n",
    "global_gate_count = 0\n",
    "def gradient_operator(circuit, q, params, grad_index, gate_count):\n",
    "    global global_gate_count\n",
    "\n",
    "    n = len(q)\n",
    "    if (n < 3):\n",
    "        global_gate_count += 1\n",
    "\n",
    "        circuit.ry(params[0], q[0])\n",
    "        if (global_gate_count == grad_index):\n",
    "            circuit.y(q[0])\n",
    "\n",
    "        if (n > 1):\n",
    "            global_gate_count += 1\n",
    "            circuit.ry(params[1], q[1])\n",
    "            circuit.cx(q[1], q[0])\n",
    "            if (global_gate_count == grad_index):\n",
    "                circuit.y(q[1])\n",
    "        \n",
    "    else:\n",
    "        k = m.ceil(n / 2)\n",
    "        gradient_operator(circuit, q[:k], params[:k], grad_index, gate_count)\n",
    "        gradient_operator(circuit, q[k:], params[k:], grad_index, gate_count)\n",
    "\n",
    "def gradient_quantum_processing(n, k, classical_data, params, draw_circuit=False, draw_filename=None):\n",
    "    global global_gate_count\n",
    "\n",
    "    q = QuantumRegister(n+1, \"q\") # +1 da ancilla\n",
    "    c = ClassicalRegister(1, \"c\")\n",
    "    circuit = QuantumCircuit(q, c)\n",
    "    \n",
    "    #circuit.h(q[n]) # coloca a ancilla em superposição uniforme.\n",
    "    circuit.ry(m.pi/2, q[n]) # coloca a ancilla em superposição uniforme.\n",
    "    circuit.rz(m.pi/2, q[n]) # multiplica o |1> por i (imaginário, mudando a fase).\n",
    "\n",
    "    qubit_encoding(circuit, q[0:n], classical_data)\n",
    "    circuit.barrier()\n",
    "\n",
    "    layer = 0\n",
    "    gate_count = 0\n",
    "    global_gate_count = 0\n",
    "    while True: # após o k-ésimo operador, incluir um operador sigma correspondente.\n",
    "        ops_count = m.ceil(n / 2**layer)\n",
    "\n",
    "        gradient_operator(circuit, q[:n:m.ceil(n / ops_count)], params[layer], k, gate_count)\n",
    "        circuit.barrier()\n",
    "\n",
    "        layer += 1\n",
    "        if (ops_count <= 1):\n",
    "            break\n",
    "    \n",
    "    #circuit.rx(-m.pi/2, q[0])\n",
    "\n",
    "    while True: # tranposto conjugado dos operadores, na sequência inversa.\n",
    "        layer -= 1\n",
    "        ops_count = m.ceil(n / 2**layer)\n",
    "\n",
    "        mry(circuit, q[:n:m.ceil(n / ops_count)], params[layer], dagger=True)\n",
    "        circuit.barrier()\n",
    "\n",
    "        if (layer == 0):\n",
    "            break\n",
    "\n",
    "    qubit_encoding(circuit, q[0:n], classical_data, dagger=True)\n",
    "    circuit.barrier()\n",
    "\n",
    "    circuit.measure(q[n], c[0])\n",
    "\n",
    "    simulator = Aer.get_backend('qasm_simulator')\n",
    "    job = execute(circuit, simulator, shots=128)\n",
    "    result = job.result()\n",
    "    counts = result.get_counts(circuit)\n",
    "\n",
    "    if (draw_circuit):\n",
    "        circuit.draw(output='latex', filename=draw_filename)\n",
    "        \n",
    "    return counts\n",
    "\n",
    "def gradient_vector(n, classical_data, params):\n",
    "    grad_vector = [row[:] for row in params]\n",
    "    k = 0\n",
    "    for i in range(len(params)):\n",
    "        for j in range(len(params[i])):\n",
    "            k += 1\n",
    "            # mede gradient_quantum_processing várias vezes para um componente k do gradiente.\n",
    "            counts = gradient_quantum_processing(n, k, classical_data, params)\n",
    "            # determina a probabilidade de obter 0 [ P(0) ].\n",
    "            probability = (counts['0'] / (counts['1']+counts['0']))\n",
    "\n",
    "            # finalmente, calcula o componente k do gradiente:\n",
    "            # d loss/d theta_k = 2*(1-2*P(0))\n",
    "            component_k = 1 - 2*probability\n",
    "\n",
    "            # armazena os componentes num array e retorna.\n",
    "            grad_vector[i][j] = component_k\n",
    "\n",
    "    return grad_vector\n",
    "\n",
    "def learn(rate, params, grad_vector, loss):\n",
    "    for i in range(len(params)):\n",
    "        for j in range(len(params[i])):\n",
    "            parameter = params[i][j]\n",
    "            grad = grad_vector[i][j]\n",
    "            \n",
    "            if (grad != 0):\n",
    "                params[i][j] = parameter - rate*(loss/grad)\n",
    "            \n",
    "    \n",
    "\n",
    "def main():\n",
    "    training_set_size = 10\n",
    "    test_set_size = 10\n",
    "    iris_desired_targets = [0, 1]\n",
    "    measure_probability_threshold = 0.75\n",
    "    loss_threshold = 0.1\n",
    "    learn_rate = 0.05\n",
    "\n",
    "    training_set, test_set = load_iris_set(training_set_size, test_set_size, iris_desired_targets)\n",
    "    unitary_parameters = initialize_unitary_parameters(len(training_set[0])-1)\n",
    "\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        iterations += 1\n",
    "        print('Iteração: ', iterations)\n",
    "\n",
    "        learned = True\n",
    "        for data in training_set:\n",
    "            counts = quantum_processing(data[0:-1], unitary_parameters)\n",
    "            \n",
    "            if not('0' in counts):\n",
    "                counts['0'] = 0\n",
    "            if not('1' in counts):\n",
    "                counts['1'] = 0\n",
    "\n",
    "            target = int(data[-1])\n",
    "            expectation_value = (counts['1'] / (counts['0'] + counts['1']))\n",
    "            probability = (counts[str(target)] / (counts['0'] + counts['1']))\n",
    "            \n",
    "            #loss = 1 - (1-2*target) * (1-2*expectation_value)\n",
    "            loss = (expectation_value-target)**2\n",
    "\n",
    "            print(\"LOSS:\", '%.2f' % (loss), end='')\n",
    "            print(\"\\tTgt:\", target, \" Cnt:\", counts, end='')\n",
    "            if (probability >= measure_probability_threshold):\n",
    "                print(\" OK :\", '%.2f' % (probability), \"%\")\n",
    "            else:\n",
    "                print(\" ERR:\", '%.2f' % (probability), \"%\")\n",
    "            \n",
    "            if (loss > loss_threshold):\n",
    "                print('learning...')\n",
    "                learned = False\n",
    "                grad_vector = gradient_vector(len(data)-1, data[0:-1], unitary_parameters)\n",
    "                learn(learn_rate, unitary_parameters, grad_vector, loss)\n",
    "\n",
    "        if (learned):\n",
    "            break\n",
    "    \n",
    "    print('Parâmetros:')\n",
    "    for i in unitary_parameters:\n",
    "        print(i, '\\t', end='')\n",
    "\n",
    "    for data in test_set:\n",
    "        counts = quantum_processing(data[0:-1], unitary_parameters)\n",
    "        \n",
    "        if not('0' in counts):\n",
    "            counts['0'] = 0\n",
    "        if not('1' in counts):\n",
    "            counts['1'] = 0\n",
    "\n",
    "        target = int(data[-1])\n",
    "        probability = (counts[str(target)] / (counts['0'] + counts['1']))\n",
    "                    \n",
    "        print(\"\\tTgt:\", target, \" Cnt:\", counts, end='')\n",
    "        if (probability >= measure_probability_threshold):\n",
    "            print(\" OK :\", '%.2f' % (probability), \"%\")\n",
    "        else:\n",
    "            print(\" ERR:\", '%.2f' % (probability), \"%\")\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prática: MERA\n",
    "Fazer MERA apenas se houver tempo.\n",
    "\n",
    "Resumo curto sobre MERA (Multi-scale Entanglement Renormalization Ansatz)\n",
    "\n",
    "### Qiskit\n",
    "Implementação utilizando Python e Qiskit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusões e Comentários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
